{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P2nAzKHtzf6g"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from typing import List, Tuple, Dict, Optional\n",
        "import time\n",
        "import warnings\n",
        "from collections import OrderedDict\n",
        "from google.colab import files\n",
        "import random\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62X7F7GUz1e9"
      },
      "outputs": [],
      "source": [
        "AMINO_ACIDS = ['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L',\n",
        "               'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y']\n",
        "\n",
        "AA_TO_IDX = {aa: idx + 1 for idx, aa in enumerate(AMINO_ACIDS)}\n",
        "AA_TO_IDX['X'] = 0\n",
        "\n",
        "# Physicochemical properties (normalized)\n",
        "AA_PROPERTIES = {\n",
        "    # Hydrophobicity (Kyte-Doolittle scale, normalized to [-1, 1])\n",
        "    'hydrophobicity': {\n",
        "        'A': 0.47, 'C': 0.67, 'D': -0.93, 'E': -0.93, 'F': 0.73,\n",
        "        'G': -0.10, 'H': -0.80, 'I': 1.13, 'K': -0.93, 'L': 0.97,\n",
        "        'M': 0.53, 'N': -0.93, 'P': -0.47, 'Q': -0.93, 'R': -1.20,\n",
        "        'S': -0.20, 'T': -0.17, 'V': 1.07, 'W': -0.27, 'Y': -0.40, 'X': 0.0\n",
        "    },\n",
        "    # Charge at pH 7\n",
        "    'charge': {\n",
        "        'A': 0, 'C': 0, 'D': -1, 'E': -1, 'F': 0,\n",
        "        'G': 0, 'H': 0.1, 'I': 0, 'K': 1, 'L': 0,\n",
        "        'M': 0, 'N': 0, 'P': 0, 'Q': 0, 'R': 1,\n",
        "        'S': 0, 'T': 0, 'V': 0, 'W': 0, 'Y': 0, 'X': 0\n",
        "    },\n",
        "    # Polarity\n",
        "    'polarity': {\n",
        "        'A': 0, 'C': 1, 'D': 1, 'E': 1, 'F': 0,\n",
        "        'G': 0, 'H': 1, 'I': 0, 'K': 1, 'L': 0,\n",
        "        'M': 0, 'N': 1, 'P': 0, 'Q': 1, 'R': 1,\n",
        "        'S': 1, 'T': 1, 'V': 0, 'W': 0, 'Y': 1, 'X': 0\n",
        "    },\n",
        "    # Size (molecular weight, normalized)\n",
        "    'size': {\n",
        "        'A': 0.11, 'C': 0.15, 'D': 0.17, 'E': 0.19, 'F': 0.21,\n",
        "        'G': 0.09, 'H': 0.20, 'I': 0.17, 'K': 0.19, 'L': 0.17,\n",
        "        'M': 0.19, 'N': 0.17, 'P': 0.15, 'Q': 0.19, 'R': 0.22,\n",
        "        'S': 0.13, 'T': 0.15, 'V': 0.15, 'W': 0.25, 'Y': 0.23, 'X': 0.17\n",
        "    }\n",
        "}\n",
        "\n",
        "def encode_sequence(sequence: str) -> List[int]:\n",
        "    \"\"\"Encode amino acid sequence to integer indices\"\"\"\n",
        "    return [AA_TO_IDX.get(aa, 0) for aa in sequence.upper()]\n",
        "\n",
        "\n",
        "def extract_physicochemical_features(sequence: str) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Extract physicochemical features from sequence:\n",
        "    - Mean, std, min, max for each property\n",
        "    - Sequence composition (AA frequencies)\n",
        "    - C-terminal and N-terminal features\n",
        "    \"\"\"\n",
        "    seq = sequence.upper()\n",
        "    features = []\n",
        "\n",
        "    # For each physicochemical property\n",
        "    for prop_name, prop_dict in AA_PROPERTIES.items():\n",
        "        values = [prop_dict.get(aa, 0) for aa in seq]\n",
        "        features.extend([\n",
        "            np.mean(values),\n",
        "            np.std(values),\n",
        "            np.min(values),\n",
        "            np.max(values)\n",
        "        ])\n",
        "\n",
        "    # Amino acid composition (20 features)\n",
        "    total = len(seq)\n",
        "    for aa in AMINO_ACIDS:\n",
        "        features.append(seq.count(aa) / total if total > 0 else 0)\n",
        "\n",
        "    # N-terminal and C-terminal features (first/last 10 residues)\n",
        "    n_term = seq[:10].ljust(10, 'X')\n",
        "    c_term = seq[-10:].rjust(10, 'X')\n",
        "\n",
        "    for aa in n_term + c_term:\n",
        "        for prop_name, prop_dict in AA_PROPERTIES.items():\n",
        "            features.append(prop_dict.get(aa, 0))\n",
        "\n",
        "    return np.array(features, dtype=np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JrDnS-chz3do"
      },
      "outputs": [],
      "source": [
        "def generate_kmers(sequence: str, k: int) -> List[str]:\n",
        "    \"\"\"Generate all k-mers from a sequence\"\"\"\n",
        "    sequence = sequence.upper()\n",
        "    return [sequence[i:i+k] for i in range(len(sequence) - k + 1)]\n",
        "\n",
        "\n",
        "def get_top_kmers(sequences: List[str], k: int, top_n: int = 150) -> List[str]:\n",
        "    \"\"\"Get top N most frequent k-mers (increased from 100 to 150)\"\"\"\n",
        "    kmer_counts = {}\n",
        "\n",
        "    for seq in sequences:\n",
        "        kmers = generate_kmers(seq, k)\n",
        "        for kmer in kmers:\n",
        "            kmer_counts[kmer] = kmer_counts.get(kmer, 0) + 1\n",
        "\n",
        "    sorted_kmers = sorted(kmer_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "    return [kmer for kmer, count in sorted_kmers[:top_n]]\n",
        "\n",
        "\n",
        "def extract_kmer_features_tfidf(sequence: str, kmer_list: List[str], k: int,\n",
        "                                 idf_weights: Optional[Dict[str, float]] = None) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Extract TF-IDF k-mer features instead of binary presence/absence\n",
        "    TF = frequency in sequence, IDF = inverse document frequency\n",
        "    \"\"\"\n",
        "    seq_kmers = generate_kmers(sequence, k)\n",
        "    kmer_counts = {}\n",
        "    for kmer in seq_kmers:\n",
        "        kmer_counts[kmer] = kmer_counts.get(kmer, 0) + 1\n",
        "\n",
        "    features = []\n",
        "    total_kmers = len(seq_kmers) if len(seq_kmers) > 0 else 1\n",
        "\n",
        "    for kmer in kmer_list:\n",
        "        tf = kmer_counts.get(kmer, 0) / total_kmers\n",
        "        if idf_weights and kmer in idf_weights:\n",
        "            features.append(tf * idf_weights[kmer])\n",
        "        else:\n",
        "            features.append(tf)\n",
        "\n",
        "    return np.array(features, dtype=np.float32)\n",
        "\n",
        "\n",
        "def calculate_idf_weights(sequences: List[str], kmer_list: List[str], k: int) -> Dict[str, float]:\n",
        "    \"\"\"Calculate IDF weights for k-mers\"\"\"\n",
        "    doc_counts = {kmer: 0 for kmer in kmer_list}\n",
        "\n",
        "    for seq in sequences:\n",
        "        seq_kmers = set(generate_kmers(seq, k))\n",
        "        for kmer in kmer_list:\n",
        "            if kmer in seq_kmers:\n",
        "                doc_counts[kmer] += 1\n",
        "\n",
        "    n_docs = len(sequences)\n",
        "    idf_weights = {}\n",
        "    for kmer, count in doc_counts.items():\n",
        "        idf_weights[kmer] = np.log(n_docs / (count + 1))\n",
        "\n",
        "    return idf_weights\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 3. DATA AUGMENTATION\n",
        "# ============================================================================\n",
        "\n",
        "def augment_sequence(sequence: str, mutation_rate: float = 0.02) -> str:\n",
        "    \"\"\"\n",
        "    Augment sequence with random mutations\n",
        "    - Conservative mutations (similar physicochemical properties)\n",
        "    \"\"\"\n",
        "    seq_list = list(sequence.upper())\n",
        "    n_mutations = max(1, int(len(seq_list) * mutation_rate))\n",
        "\n",
        "    # Conservative mutation groups\n",
        "    conservative_groups = [\n",
        "        ['A', 'G', 'S', 'T'],  # Small, polar\n",
        "        ['D', 'E'],  # Acidic\n",
        "        ['K', 'R', 'H'],  # Basic\n",
        "        ['I', 'L', 'V', 'M'],  # Hydrophobic\n",
        "        ['F', 'Y', 'W'],  # Aromatic\n",
        "        ['N', 'Q'],  # Amide\n",
        "    ]\n",
        "\n",
        "    for _ in range(n_mutations):\n",
        "        pos = random.randint(0, len(seq_list) - 1)\n",
        "        original_aa = seq_list[pos]\n",
        "\n",
        "        # Find conservative replacement\n",
        "        for group in conservative_groups:\n",
        "            if original_aa in group and len(group) > 1:\n",
        "                replacements = [aa for aa in group if aa != original_aa]\n",
        "                seq_list[pos] = random.choice(replacements)\n",
        "                break\n",
        "\n",
        "    return ''.join(seq_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ed8P95c3z8cA"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"Multi-head self-attention mechanism\"\"\"\n",
        "\n",
        "    def __init__(self, embed_dim: int, num_heads: int = 8, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        assert embed_dim % num_heads == 0\n",
        "\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        self.q_linear = nn.Linear(embed_dim, embed_dim)\n",
        "        self.k_linear = nn.Linear(embed_dim, embed_dim)\n",
        "        self.v_linear = nn.Linear(embed_dim, embed_dim)\n",
        "        self.out_linear = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.layer_norm = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, embed_dim)\n",
        "        # Add sequence dimension\n",
        "        x = x.unsqueeze(1)  # (batch, 1, embed_dim)\n",
        "\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # Linear projections\n",
        "        Q = self.q_linear(x).view(batch_size, 1, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        K = self.k_linear(x).view(batch_size, 1, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        V = self.v_linear(x).view(batch_size, 1, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # Attention scores\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.head_dim)\n",
        "        attn_weights = F.softmax(scores, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        # Apply attention to values\n",
        "        attn_output = torch.matmul(attn_weights, V)\n",
        "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, 1, self.embed_dim)\n",
        "\n",
        "        # Output projection\n",
        "        output = self.out_linear(attn_output)\n",
        "        output = output.squeeze(1)  # (batch, embed_dim)\n",
        "\n",
        "        # Residual connection and layer norm\n",
        "        output = self.layer_norm(x.squeeze(1) + self.dropout(output))\n",
        "\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TDua_jen0AQp"
      },
      "outputs": [],
      "source": [
        "class FocalLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Focal Loss for addressing class imbalance\n",
        "    Focuses training on hard examples\n",
        "    \"\"\"\n",
        "    def __init__(self, alpha: float = 0.25, gamma: float = 2.0):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
        "        pt = torch.exp(-ce_loss)\n",
        "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
        "        return focal_loss.mean()\n",
        "\n",
        "\n",
        "class ImprovedBacteriocinClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    Improved Bacteriocin Classifier with:\n",
        "    - Multi-head attention\n",
        "    - Residual connections\n",
        "    - Batch normalization\n",
        "    - Deeper architecture\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim: int, hidden_dims: List[int] = [512, 256, 128, 64]):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_projection = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dims[0]),\n",
        "            nn.BatchNorm1d(hidden_dims[0]),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3)\n",
        "        )\n",
        "\n",
        "        # Multi-head attention\n",
        "        self.attention = MultiHeadAttention(hidden_dims[0], num_heads=8, dropout=0.2)\n",
        "\n",
        "        # Deep residual blocks\n",
        "        self.blocks = nn.ModuleList()\n",
        "        for i in range(len(hidden_dims) - 1):\n",
        "            self.blocks.append(\n",
        "                nn.Sequential(\n",
        "                    nn.Linear(hidden_dims[i], hidden_dims[i+1]),\n",
        "                    nn.BatchNorm1d(hidden_dims[i+1]),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Dropout(0.3)\n",
        "                )\n",
        "            )\n",
        "\n",
        "        # Output layer\n",
        "        self.output = nn.Sequential(\n",
        "            nn.Linear(hidden_dims[-1], 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(32, 2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input projection\n",
        "        x = self.input_projection(x)\n",
        "\n",
        "        # Self-attention\n",
        "        x = self.attention(x)\n",
        "\n",
        "        # Deep blocks\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "\n",
        "        # Output\n",
        "        x = self.output(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOuRxBWQ0DTm"
      },
      "outputs": [],
      "source": [
        "class ImprovedBacteriocinDataset(Dataset):\n",
        "    \"\"\"Dataset with optional augmentation\"\"\"\n",
        "\n",
        "    def __init__(self, features: np.ndarray, labels: np.ndarray,\n",
        "                 sequences: Optional[List[str]] = None,\n",
        "                 kmer_lists: Optional[Dict] = None,\n",
        "                 idf_weights: Optional[Dict] = None,\n",
        "                 augment: bool = False):\n",
        "        self.features = torch.FloatTensor(features)\n",
        "        self.labels = torch.LongTensor(labels)\n",
        "        self.sequences = sequences\n",
        "        self.kmer_lists = kmer_lists\n",
        "        self.idf_weights = idf_weights\n",
        "        self.augment = augment\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        features = self.features[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Optional: augment on-the-fly (only for training)\n",
        "        if self.augment and self.sequences and random.random() < 0.3:\n",
        "            # Re-extract features from augmented sequence\n",
        "            aug_seq = augment_sequence(self.sequences[idx])\n",
        "            # Note: In practice, this would need the full feature extraction pipeline\n",
        "            # For now, we'll just use original features\n",
        "            pass\n",
        "\n",
        "        return features, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1UzrSv7-0HRU"
      },
      "outputs": [],
      "source": [
        "def train_epoch_improved(model, dataloader, criterion, optimizer, device,\n",
        "                         clip_grad: float = 1.0):\n",
        "    \"\"\"Train with gradient clipping\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for features, labels in dataloader:\n",
        "        features, labels = features.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n",
        "\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "\n",
        "def evaluate_improved(model, dataloader, criterion, device):\n",
        "    \"\"\"Enhanced evaluation with more metrics\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_probs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for features, labels in dataloader:\n",
        "            features, labels = features.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(features)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            probs = F.softmax(outputs, dim=1)\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_probs.extend(probs[:, 1].cpu().numpy())\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision = precision_score(all_labels, all_preds, average='binary', zero_division=0)\n",
        "    recall = recall_score(all_labels, all_preds, average='binary', zero_division=0)\n",
        "    f1 = f1_score(all_labels, all_preds, average='binary', zero_division=0)\n",
        "\n",
        "    try:\n",
        "        auc = roc_auc_score(all_labels, all_probs)\n",
        "    except:\n",
        "        auc = 0.0\n",
        "\n",
        "    return avg_loss, accuracy, precision, recall, f1, auc, all_preds, all_labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N6zbwIvo0Lyk"
      },
      "outputs": [],
      "source": [
        "def kfold_cross_validation_improved(features: np.ndarray,\n",
        "                                    labels: np.ndarray,\n",
        "                                    sequences: Optional[List[str]] = None,\n",
        "                                    k: int = 30,  # Reduced from 30 for faster training\n",
        "                                    epochs: int = 100,\n",
        "                                    batch_size: int = 64,  # Increased from 40\n",
        "                                    learning_rate: float = 1e-4,  # Higher initial LR\n",
        "                                    device: str = 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "                                    patience: int = 15):\n",
        "    \"\"\"\n",
        "    Improved k-fold cross validation with:\n",
        "    - Stratified splits\n",
        "    - Early stopping\n",
        "    - Cosine annealing LR schedule\n",
        "    - Focal loss\n",
        "    \"\"\"\n",
        "\n",
        "    # Use StratifiedKFold to maintain class balance\n",
        "    kfold = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
        "    fold_results = []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kfold.split(features, labels)):\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Fold {fold + 1}/{k}\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        # Split data\n",
        "        X_train, X_val = features[train_idx], features[val_idx]\n",
        "        y_train, y_val = labels[train_idx], labels[val_idx]\n",
        "\n",
        "        # Create datasets\n",
        "        train_dataset = ImprovedBacteriocinDataset(X_train, y_train)\n",
        "        val_dataset = ImprovedBacteriocinDataset(X_val, y_val)\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "        # Initialize model\n",
        "        model = ImprovedBacteriocinClassifier(\n",
        "            input_dim=features.shape[1],\n",
        "            hidden_dims=[512, 256, 128, 64]\n",
        "        ).to(device)\n",
        "\n",
        "        # Focal loss for better handling of hard examples\n",
        "        criterion = FocalLoss(alpha=0.25, gamma=2.0)\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
        "\n",
        "        # Cosine annealing learning rate scheduler\n",
        "        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "            optimizer, T_0=10, T_mult=2, eta_min=1e-6\n",
        "        )\n",
        "\n",
        "        # Training history\n",
        "        train_losses = []\n",
        "        val_losses = []\n",
        "        val_accuracies = []\n",
        "        val_f1_scores = []\n",
        "\n",
        "        best_f1 = 0\n",
        "        patience_counter = 0\n",
        "        best_model_state = None\n",
        "\n",
        "        # Training loop with early stopping\n",
        "        for epoch in range(epochs):\n",
        "            train_loss = train_epoch_improved(model, train_loader, criterion, optimizer, device)\n",
        "            val_loss, val_acc, val_prec, val_rec, val_f1, val_auc, _, _ = evaluate_improved(\n",
        "                model, val_loader, criterion, device\n",
        "            )\n",
        "\n",
        "            train_losses.append(train_loss)\n",
        "            val_losses.append(val_loss)\n",
        "            val_accuracies.append(val_acc)\n",
        "            val_f1_scores.append(val_f1)\n",
        "\n",
        "            scheduler.step()\n",
        "\n",
        "            # Early stopping based on F1 score\n",
        "            if val_f1 > best_f1:\n",
        "                best_f1 = val_f1\n",
        "                patience_counter = 0\n",
        "                best_model_state = model.state_dict().copy()\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "\n",
        "            if (epoch + 1) % 10 == 0:\n",
        "                print(f\"Epoch {epoch+1}/{epochs} - \"\n",
        "                      f\"Train Loss: {train_loss:.4f}, \"\n",
        "                      f\"Val Loss: {val_loss:.4f}, \"\n",
        "                      f\"Val Acc: {val_acc:.4f}, \"\n",
        "                      f\"Val F1: {val_f1:.4f}, \"\n",
        "                      f\"Val AUC: {val_auc:.4f}\")\n",
        "\n",
        "            if patience_counter >= patience:\n",
        "                print(f\"Early stopping at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "        # Load best model\n",
        "        if best_model_state:\n",
        "            model.load_state_dict(best_model_state)\n",
        "\n",
        "        # Final evaluation\n",
        "        final_loss, final_acc, final_prec, final_rec, final_f1, final_auc, preds, true_labels = evaluate_improved(\n",
        "            model, val_loader, criterion, device\n",
        "        )\n",
        "\n",
        "        fold_results.append({\n",
        "            'fold': fold + 1,\n",
        "            'loss': final_loss,\n",
        "            'accuracy': final_acc * 100,\n",
        "            'precision': final_prec,\n",
        "            'recall': final_rec,\n",
        "            'f1_score': final_f1,\n",
        "            'auc': final_auc,\n",
        "            'train_losses': train_losses,\n",
        "            'val_losses': val_losses,\n",
        "            'val_accuracies': val_accuracies,\n",
        "            'val_f1_scores': val_f1_scores,\n",
        "            'predictions': preds,\n",
        "            'true_labels': true_labels,\n",
        "            'model_state': model.state_dict()\n",
        "        })\n",
        "\n",
        "        print(f\"\\nFold {fold + 1} Results:\")\n",
        "        print(f\"Loss: {final_loss:.4f}\")\n",
        "        print(f\"Accuracy: {final_acc * 100:.2f}%\")\n",
        "        print(f\"Precision: {final_prec:.4f}\")\n",
        "        print(f\"Recall: {final_rec:.4f}\")\n",
        "        print(f\"F1 Score: {final_f1:.4f}\")\n",
        "        print(f\"AUC: {final_auc:.4f}\")\n",
        "\n",
        "    return fold_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "msNoQ7vD0OMv"
      },
      "outputs": [],
      "source": [
        "def ensemble_predict(models: List[nn.Module], features: np.ndarray, device: str) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Ensemble prediction using multiple models\n",
        "    Returns averaged probabilities\n",
        "    \"\"\"\n",
        "    all_probs = []\n",
        "\n",
        "    features_tensor = torch.FloatTensor(features).to(device)\n",
        "\n",
        "    for model in models:\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            outputs = model(features_tensor)\n",
        "            probs = F.softmax(outputs, dim=1)\n",
        "            all_probs.append(probs.cpu().numpy())\n",
        "\n",
        "    # Average probabilities\n",
        "    avg_probs = np.mean(all_probs, axis=0)\n",
        "    predictions = np.argmax(avg_probs, axis=1)\n",
        "\n",
        "    return predictions, avg_probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWnajunB0PJZ",
        "outputId": "eb83ca5f-82aa-4242-f3a1-d98b75e1d191"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "BacLABNet v2.0: Improved Bacteriocin Classification\n",
            "======================================================================\n",
            "\n",
            "[1/6] Loading data...\n",
            "Total sequences: 49964\n",
            "BacLAB: 24964, Non-BacLAB: 25000\n",
            "\n",
            "[2/6] Extracting improved k-mer features (TF-IDF)...\n",
            "Computing k-mers...\n",
            "Calculating TF-IDF weights...\n",
            "5-mer TF-IDF features: (49964, 150)\n",
            "7-mer TF-IDF features: (49964, 150)\n",
            "\n",
            "[3/6] Extracting physicochemical features...\n",
            "Physicochemical features: (49964, 116)\n",
            "Time: 34.49 seconds\n",
            "\n",
            "[4/6] Loading embedding features...\n",
            "✓ Loaded ESM-2 embeddings: (49964, 480)\n",
            "  (Using state-of-the-art ESM-2 protein language model embeddings)\n",
            "\n",
            "[5/6] Concatenating features...\n",
            "Final features shape: (49964, 896)\n",
            "Feature breakdown:\n",
            "  - 5-mer TF-IDF: 150\n",
            "  - 7-mer TF-IDF: 150\n",
            "  - Physicochemical: 116\n",
            "  - Embeddings: 480\n",
            "  - Total: 896\n",
            "\n",
            "[6/6] Starting improved k-fold cross-validation...\n",
            "Improvements:\n",
            "  ✓ Transformer architecture with multi-head attention\n",
            "  ✓ Focal loss for better hard example learning\n",
            "  ✓ Cosine annealing learning rate schedule\n",
            "  ✓ Early stopping with patience=15\n",
            "  ✓ Gradient clipping\n",
            "  ✓ Deeper network (512→256→128→64→32→2)\n",
            "  Device: CUDA\n",
            "\n",
            "============================================================\n",
            "Fold 1/30\n",
            "============================================================\n",
            "Epoch 10/100 - Train Loss: 0.0092, Val Loss: 0.0110, Val Acc: 0.9376, Val F1: 0.9390, Val AUC: 0.9838\n",
            "Epoch 20/100 - Train Loss: 0.0066, Val Loss: 0.0097, Val Acc: 0.9460, Val F1: 0.9470, Val AUC: 0.9878\n",
            "Epoch 30/100 - Train Loss: 0.0046, Val Loss: 0.0107, Val Acc: 0.9502, Val F1: 0.9508, Val AUC: 0.9887\n",
            "Epoch 40/100 - Train Loss: 0.0045, Val Loss: 0.0126, Val Acc: 0.9526, Val F1: 0.9534, Val AUC: 0.9871\n",
            "Epoch 50/100 - Train Loss: 0.0030, Val Loss: 0.0140, Val Acc: 0.9556, Val F1: 0.9560, Val AUC: 0.9881\n",
            "Epoch 60/100 - Train Loss: 0.0019, Val Loss: 0.0182, Val Acc: 0.9544, Val F1: 0.9550, Val AUC: 0.9878\n",
            "Epoch 70/100 - Train Loss: 0.0016, Val Loss: 0.0196, Val Acc: 0.9550, Val F1: 0.9556, Val AUC: 0.9880\n",
            "Epoch 80/100 - Train Loss: 0.0027, Val Loss: 0.0163, Val Acc: 0.9532, Val F1: 0.9541, Val AUC: 0.9878\n",
            "Early stopping at epoch 87\n",
            "\n",
            "Fold 1 Results:\n",
            "Loss: 0.0157\n",
            "Accuracy: 95.14%\n",
            "Precision: 0.9342\n",
            "Recall: 0.9712\n",
            "F1 Score: 0.9523\n",
            "AUC: 0.9881\n",
            "\n",
            "============================================================\n",
            "Fold 2/30\n",
            "============================================================\n",
            "Epoch 10/100 - Train Loss: 0.0096, Val Loss: 0.0113, Val Acc: 0.9352, Val F1: 0.9359, Val AUC: 0.9840\n",
            "Epoch 20/100 - Train Loss: 0.0065, Val Loss: 0.0136, Val Acc: 0.9460, Val F1: 0.9466, Val AUC: 0.9885\n",
            "Epoch 30/100 - Train Loss: 0.0046, Val Loss: 0.0118, Val Acc: 0.9496, Val F1: 0.9498, Val AUC: 0.9895\n",
            "Epoch 40/100 - Train Loss: 0.0047, Val Loss: 0.0126, Val Acc: 0.9472, Val F1: 0.9475, Val AUC: 0.9888\n",
            "Epoch 50/100 - Train Loss: 0.0031, Val Loss: 0.0140, Val Acc: 0.9568, Val F1: 0.9572, Val AUC: 0.9903\n",
            "Epoch 60/100 - Train Loss: 0.0020, Val Loss: 0.0220, Val Acc: 0.9562, Val F1: 0.9567, Val AUC: 0.9903\n",
            "Epoch 70/100 - Train Loss: 0.0016, Val Loss: 0.0230, Val Acc: 0.9562, Val F1: 0.9567, Val AUC: 0.9906\n",
            "Epoch 80/100 - Train Loss: 0.0029, Val Loss: 0.0175, Val Acc: 0.9550, Val F1: 0.9553, Val AUC: 0.9905\n",
            "Early stopping at epoch 80\n",
            "\n",
            "Fold 2 Results:\n",
            "Loss: 0.0175\n",
            "Accuracy: 95.50%\n",
            "Precision: 0.9480\n",
            "Recall: 0.9628\n",
            "F1 Score: 0.9553\n",
            "AUC: 0.9905\n",
            "\n",
            "============================================================\n",
            "Fold 3/30\n",
            "============================================================\n",
            "Epoch 10/100 - Train Loss: 0.0093, Val Loss: 0.0099, Val Acc: 0.9508, Val F1: 0.9514, Val AUC: 0.9845\n",
            "Epoch 20/100 - Train Loss: 0.0064, Val Loss: 0.0107, Val Acc: 0.9508, Val F1: 0.9514, Val AUC: 0.9864\n",
            "Epoch 30/100 - Train Loss: 0.0045, Val Loss: 0.0122, Val Acc: 0.9550, Val F1: 0.9552, Val AUC: 0.9873\n",
            "Epoch 40/100 - Train Loss: 0.0045, Val Loss: 0.0130, Val Acc: 0.9544, Val F1: 0.9548, Val AUC: 0.9869\n",
            "Epoch 50/100 - Train Loss: 0.0029, Val Loss: 0.0160, Val Acc: 0.9634, Val F1: 0.9638, Val AUC: 0.9866\n",
            "Epoch 60/100 - Train Loss: 0.0021, Val Loss: 0.0200, Val Acc: 0.9616, Val F1: 0.9620, Val AUC: 0.9854\n",
            "Early stopping at epoch 65\n",
            "\n",
            "Fold 3 Results:\n",
            "Loss: 0.0196\n",
            "Accuracy: 96.16%\n",
            "Precision: 0.9529\n",
            "Recall: 0.9712\n",
            "F1 Score: 0.9620\n",
            "AUC: 0.9861\n",
            "\n",
            "============================================================\n",
            "Fold 4/30\n",
            "============================================================\n",
            "Epoch 10/100 - Train Loss: 0.0092, Val Loss: 0.0103, Val Acc: 0.9406, Val F1: 0.9425, Val AUC: 0.9836\n",
            "Epoch 20/100 - Train Loss: 0.0063, Val Loss: 0.0120, Val Acc: 0.9466, Val F1: 0.9479, Val AUC: 0.9845\n",
            "Epoch 30/100 - Train Loss: 0.0045, Val Loss: 0.0128, Val Acc: 0.9520, Val F1: 0.9531, Val AUC: 0.9868\n",
            "Epoch 40/100 - Train Loss: 0.0047, Val Loss: 0.0127, Val Acc: 0.9490, Val F1: 0.9499, Val AUC: 0.9861\n",
            "Early stopping at epoch 48\n",
            "\n",
            "Fold 4 Results:\n",
            "Loss: 0.0145\n",
            "Accuracy: 95.62%\n",
            "Precision: 0.9408\n",
            "Recall: 0.9736\n",
            "F1 Score: 0.9569\n",
            "AUC: 0.9875\n",
            "\n",
            "============================================================\n",
            "Fold 5/30\n",
            "============================================================\n",
            "Epoch 10/100 - Train Loss: 0.0097, Val Loss: 0.0083, Val Acc: 0.9526, Val F1: 0.9538, Val AUC: 0.9908\n",
            "Epoch 20/100 - Train Loss: 0.0067, Val Loss: 0.0085, Val Acc: 0.9634, Val F1: 0.9635, Val AUC: 0.9909\n",
            "Epoch 30/100 - Train Loss: 0.0048, Val Loss: 0.0089, Val Acc: 0.9646, Val F1: 0.9648, Val AUC: 0.9920\n",
            "Early stopping at epoch 38\n",
            "\n",
            "Fold 5 Results:\n",
            "Loss: 0.0095\n",
            "Accuracy: 96.76%\n",
            "Precision: 0.9664\n",
            "Recall: 0.9688\n",
            "F1 Score: 0.9676\n",
            "AUC: 0.9909\n",
            "\n",
            "============================================================\n",
            "Fold 6/30\n",
            "============================================================\n",
            "Epoch 10/100 - Train Loss: 0.0095, Val Loss: 0.0095, Val Acc: 0.9496, Val F1: 0.9504, Val AUC: 0.9862\n",
            "Epoch 20/100 - Train Loss: 0.0068, Val Loss: 0.0087, Val Acc: 0.9514, Val F1: 0.9520, Val AUC: 0.9891\n",
            "Epoch 30/100 - Train Loss: 0.0045, Val Loss: 0.0099, Val Acc: 0.9568, Val F1: 0.9573, Val AUC: 0.9896\n",
            "Epoch 40/100 - Train Loss: 0.0045, Val Loss: 0.0102, Val Acc: 0.9610, Val F1: 0.9617, Val AUC: 0.9898\n",
            "Epoch 50/100 - Train Loss: 0.0031, Val Loss: 0.0138, Val Acc: 0.9550, Val F1: 0.9554, Val AUC: 0.9886\n",
            "Early stopping at epoch 55\n",
            "\n",
            "Fold 6 Results:\n",
            "Loss: 0.0152\n",
            "Accuracy: 95.80%\n",
            "Precision: 0.9451\n",
            "Recall: 0.9724\n",
            "F1 Score: 0.9585\n",
            "AUC: 0.9890\n",
            "\n",
            "============================================================\n",
            "Fold 7/30\n",
            "============================================================\n",
            "Epoch 10/100 - Train Loss: 0.0092, Val Loss: 0.0097, Val Acc: 0.9496, Val F1: 0.9508, Val AUC: 0.9864\n",
            "Epoch 20/100 - Train Loss: 0.0064, Val Loss: 0.0087, Val Acc: 0.9514, Val F1: 0.9521, Val AUC: 0.9901\n",
            "Epoch 30/100 - Train Loss: 0.0046, Val Loss: 0.0106, Val Acc: 0.9538, Val F1: 0.9542, Val AUC: 0.9896\n",
            "Epoch 40/100 - Train Loss: 0.0047, Val Loss: 0.0120, Val Acc: 0.9568, Val F1: 0.9570, Val AUC: 0.9878\n",
            "Epoch 50/100 - Train Loss: 0.0031, Val Loss: 0.0122, Val Acc: 0.9568, Val F1: 0.9574, Val AUC: 0.9898\n",
            "Early stopping at epoch 53\n",
            "\n",
            "Fold 7 Results:\n",
            "Loss: 0.0132\n",
            "Accuracy: 96.04%\n",
            "Precision: 0.9453\n",
            "Recall: 0.9772\n",
            "F1 Score: 0.9610\n",
            "AUC: 0.9909\n",
            "\n",
            "============================================================\n",
            "Fold 8/30\n",
            "============================================================\n",
            "Epoch 10/100 - Train Loss: 0.0093, Val Loss: 0.0114, Val Acc: 0.9340, Val F1: 0.9354, Val AUC: 0.9813\n",
            "Epoch 20/100 - Train Loss: 0.0065, Val Loss: 0.0126, Val Acc: 0.9466, Val F1: 0.9477, Val AUC: 0.9832\n",
            "Epoch 30/100 - Train Loss: 0.0047, Val Loss: 0.0151, Val Acc: 0.9508, Val F1: 0.9520, Val AUC: 0.9855\n",
            "Epoch 40/100 - Train Loss: 0.0045, Val Loss: 0.0159, Val Acc: 0.9514, Val F1: 0.9522, Val AUC: 0.9846\n",
            "Early stopping at epoch 42\n",
            "\n",
            "Fold 8 Results:\n",
            "Loss: 0.0189\n",
            "Accuracy: 94.72%\n",
            "Precision: 0.9208\n",
            "Recall: 0.9784\n",
            "F1 Score: 0.9487\n",
            "AUC: 0.9842\n",
            "\n",
            "============================================================\n",
            "Fold 9/30\n",
            "============================================================\n",
            "Epoch 10/100 - Train Loss: 0.0094, Val Loss: 0.0085, Val Acc: 0.9532, Val F1: 0.9540, Val AUC: 0.9885\n",
            "Epoch 20/100 - Train Loss: 0.0063, Val Loss: 0.0083, Val Acc: 0.9562, Val F1: 0.9570, Val AUC: 0.9904\n",
            "Epoch 30/100 - Train Loss: 0.0045, Val Loss: 0.0098, Val Acc: 0.9598, Val F1: 0.9607, Val AUC: 0.9913\n",
            "Early stopping at epoch 36\n",
            "\n",
            "Fold 9 Results:\n",
            "Loss: 0.0087\n",
            "Accuracy: 95.92%\n",
            "Precision: 0.9432\n",
            "Recall: 0.9772\n",
            "F1 Score: 0.9599\n",
            "AUC: 0.9912\n",
            "\n",
            "============================================================\n",
            "Fold 10/30\n",
            "============================================================\n",
            "Epoch 10/100 - Train Loss: 0.0092, Val Loss: 0.0102, Val Acc: 0.9406, Val F1: 0.9424, Val AUC: 0.9863\n",
            "Epoch 20/100 - Train Loss: 0.0065, Val Loss: 0.0103, Val Acc: 0.9502, Val F1: 0.9517, Val AUC: 0.9895\n",
            "Epoch 30/100 - Train Loss: 0.0046, Val Loss: 0.0109, Val Acc: 0.9532, Val F1: 0.9542, Val AUC: 0.9900\n",
            "Epoch 40/100 - Train Loss: 0.0047, Val Loss: 0.0110, Val Acc: 0.9550, Val F1: 0.9559, Val AUC: 0.9894\n",
            "Epoch 50/100 - Train Loss: 0.0031, Val Loss: 0.0149, Val Acc: 0.9538, Val F1: 0.9549, Val AUC: 0.9881\n",
            "Epoch 60/100 - Train Loss: 0.0019, Val Loss: 0.0166, Val Acc: 0.9586, Val F1: 0.9594, Val AUC: 0.9891\n",
            "Early stopping at epoch 66\n",
            "\n",
            "Fold 10 Results:\n",
            "Loss: 0.0168\n",
            "Accuracy: 95.62%\n",
            "Precision: 0.9367\n",
            "Recall: 0.9784\n",
            "F1 Score: 0.9571\n",
            "AUC: 0.9895\n",
            "\n",
            "============================================================\n",
            "Fold 11/30\n",
            "============================================================\n",
            "Epoch 10/100 - Train Loss: 0.0093, Val Loss: 0.0124, Val Acc: 0.9484, Val F1: 0.9493, Val AUC: 0.9876\n",
            "Epoch 20/100 - Train Loss: 0.0065, Val Loss: 0.0183, Val Acc: 0.9574, Val F1: 0.9582, Val AUC: 0.9899\n",
            "Epoch 30/100 - Train Loss: 0.0047, Val Loss: 0.0232, Val Acc: 0.9598, Val F1: 0.9605, Val AUC: 0.9907\n",
            "Epoch 40/100 - Train Loss: 0.0046, Val Loss: 0.0223, Val Acc: 0.9586, Val F1: 0.9592, Val AUC: 0.9890\n",
            "Early stopping at epoch 42\n",
            "\n",
            "Fold 11 Results:\n",
            "Loss: 0.0266\n",
            "Accuracy: 96.22%\n",
            "Precision: 0.9529\n",
            "Recall: 0.9724\n",
            "F1 Score: 0.9625\n",
            "AUC: 0.9904\n",
            "\n",
            "============================================================\n",
            "Fold 12/30\n",
            "============================================================\n",
            "Epoch 10/100 - Train Loss: 0.0093, Val Loss: 0.0081, Val Acc: 0.9508, Val F1: 0.9518, Val AUC: 0.9898\n",
            "Epoch 20/100 - Train Loss: 0.0065, Val Loss: 0.0071, Val Acc: 0.9640, Val F1: 0.9641, Val AUC: 0.9927\n",
            "Epoch 30/100 - Train Loss: 0.0047, Val Loss: 0.0080, Val Acc: 0.9658, Val F1: 0.9661, Val AUC: 0.9926\n",
            "Early stopping at epoch 39\n",
            "\n",
            "Fold 12 Results:\n",
            "Loss: 0.0082\n",
            "Accuracy: 96.64%\n",
            "Precision: 0.9652\n",
            "Recall: 0.9675\n",
            "F1 Score: 0.9664\n",
            "AUC: 0.9925\n",
            "\n",
            "============================================================\n",
            "Fold 13/30\n",
            "============================================================\n",
            "Epoch 10/100 - Train Loss: 0.0094, Val Loss: 0.0082, Val Acc: 0.9538, Val F1: 0.9546, Val AUC: 0.9900\n",
            "Epoch 20/100 - Train Loss: 0.0065, Val Loss: 0.0083, Val Acc: 0.9628, Val F1: 0.9631, Val AUC: 0.9921\n",
            "Epoch 30/100 - Train Loss: 0.0046, Val Loss: 0.0087, Val Acc: 0.9604, Val F1: 0.9606, Val AUC: 0.9929\n",
            "Early stopping at epoch 33\n",
            "\n",
            "Fold 13 Results:\n",
            "Loss: 0.0085\n",
            "Accuracy: 96.28%\n",
            "Precision: 0.9572\n",
            "Recall: 0.9688\n",
            "F1 Score: 0.9630\n",
            "AUC: 0.9920\n",
            "\n",
            "============================================================\n",
            "Fold 14/30\n",
            "============================================================\n",
            "Epoch 10/100 - Train Loss: 0.0094, Val Loss: 0.0088, Val Acc: 0.9490, Val F1: 0.9494, Val AUC: 0.9888\n",
            "Epoch 20/100 - Train Loss: 0.0066, Val Loss: 0.0086, Val Acc: 0.9562, Val F1: 0.9568, Val AUC: 0.9905\n",
            "Epoch 30/100 - Train Loss: 0.0047, Val Loss: 0.0092, Val Acc: 0.9622, Val F1: 0.9624, Val AUC: 0.9912\n",
            "Epoch 40/100 - Train Loss: 0.0045, Val Loss: 0.0107, Val Acc: 0.9586, Val F1: 0.9591, Val AUC: 0.9893\n",
            "Early stopping at epoch 45\n",
            "\n",
            "Fold 14 Results:\n",
            "Loss: 0.0116\n",
            "Accuracy: 95.68%\n",
            "Precision: 0.9419\n",
            "Recall: 0.9736\n",
            "F1 Score: 0.9574\n",
            "AUC: 0.9910\n",
            "\n",
            "============================================================\n",
            "Fold 15/30\n",
            "============================================================\n",
            "Epoch 10/100 - Train Loss: 0.0092, Val Loss: 0.0083, Val Acc: 0.9562, Val F1: 0.9572, Val AUC: 0.9892\n",
            "Epoch 20/100 - Train Loss: 0.0064, Val Loss: 0.0084, Val Acc: 0.9634, Val F1: 0.9637, Val AUC: 0.9900\n",
            "Epoch 30/100 - Train Loss: 0.0047, Val Loss: 0.0098, Val Acc: 0.9652, Val F1: 0.9654, Val AUC: 0.9903\n",
            "Early stopping at epoch 32\n",
            "\n",
            "Fold 15 Results:\n",
            "Loss: 0.0096\n",
            "Accuracy: 96.28%\n",
            "Precision: 0.9529\n",
            "Recall: 0.9736\n",
            "F1 Score: 0.9631\n",
            "AUC: 0.9896\n",
            "\n",
            "============================================================\n",
            "Fold 16/30\n",
            "============================================================\n",
            "Epoch 10/100 - Train Loss: 0.0092, Val Loss: 0.0251, Val Acc: 0.9489, Val F1: 0.9500, Val AUC: 0.9850\n",
            "Epoch 20/100 - Train Loss: 0.0064, Val Loss: 0.0254, Val Acc: 0.9544, Val F1: 0.9548, Val AUC: 0.9898\n",
            "Epoch 30/100 - Train Loss: 0.0047, Val Loss: 0.0332, Val Acc: 0.9598, Val F1: 0.9604, Val AUC: 0.9905\n",
            "Epoch 40/100 - Train Loss: 0.0047, Val Loss: 0.0255, Val Acc: 0.9580, Val F1: 0.9585, Val AUC: 0.9911\n",
            "Early stopping at epoch 41\n",
            "\n",
            "Fold 16 Results:\n",
            "Loss: 0.0313\n",
            "Accuracy: 95.56%\n",
            "Precision: 0.9428\n",
            "Recall: 0.9700\n",
            "F1 Score: 0.9562\n",
            "AUC: 0.9900\n",
            "\n",
            "============================================================\n",
            "Fold 17/30\n",
            "============================================================\n",
            "Epoch 10/100 - Train Loss: 0.0092, Val Loss: 0.0100, Val Acc: 0.9465, Val F1: 0.9480, Val AUC: 0.9861\n",
            "Epoch 20/100 - Train Loss: 0.0064, Val Loss: 0.0091, Val Acc: 0.9556, Val F1: 0.9560, Val AUC: 0.9894\n",
            "Epoch 30/100 - Train Loss: 0.0045, Val Loss: 0.0102, Val Acc: 0.9574, Val F1: 0.9584, Val AUC: 0.9904\n",
            "Epoch 40/100 - Train Loss: 0.0045, Val Loss: 0.0117, Val Acc: 0.9544, Val F1: 0.9550, Val AUC: 0.9889\n",
            "Epoch 50/100 - Train Loss: 0.0029, Val Loss: 0.0125, Val Acc: 0.9622, Val F1: 0.9624, Val AUC: 0.9902\n",
            "Epoch 60/100 - Train Loss: 0.0020, Val Loss: 0.0160, Val Acc: 0.9634, Val F1: 0.9640, Val AUC: 0.9902\n",
            "Epoch 70/100 - Train Loss: 0.0016, Val Loss: 0.0157, Val Acc: 0.9652, Val F1: 0.9656, Val AUC: 0.9902\n",
            "Early stopping at epoch 71\n",
            "\n",
            "Fold 17 Results:\n",
            "Loss: 0.0160\n",
            "Accuracy: 95.68%\n",
            "Precision: 0.9471\n",
            "Recall: 0.9675\n",
            "F1 Score: 0.9572\n",
            "AUC: 0.9879\n",
            "\n",
            "============================================================\n",
            "Fold 18/30\n",
            "============================================================\n",
            "Epoch 10/100 - Train Loss: 0.0092, Val Loss: 0.0099, Val Acc: 0.9441, Val F1: 0.9453, Val AUC: 0.9852\n",
            "Epoch 20/100 - Train Loss: 0.0064, Val Loss: 0.0114, Val Acc: 0.9471, Val F1: 0.9463, Val AUC: 0.9866\n",
            "Epoch 30/100 - Train Loss: 0.0045, Val Loss: 0.0115, Val Acc: 0.9592, Val F1: 0.9597, Val AUC: 0.9885\n",
            "Epoch 40/100 - Train Loss: 0.0046, Val Loss: 0.0119, Val Acc: 0.9580, Val F1: 0.9587, Val AUC: 0.9883\n",
            "Early stopping at epoch 43\n",
            "\n",
            "Fold 18 Results:\n",
            "Loss: 0.0126\n",
            "Accuracy: 95.80%\n",
            "Precision: 0.9472\n",
            "Recall: 0.9700\n",
            "F1 Score: 0.9584\n",
            "AUC: 0.9888\n",
            "\n",
            "============================================================\n",
            "Fold 19/30\n",
            "============================================================\n",
            "Epoch 10/100 - Train Loss: 0.0092, Val Loss: 0.0099, Val Acc: 0.9477, Val F1: 0.9486, Val AUC: 0.9891\n",
            "Epoch 20/100 - Train Loss: 0.0065, Val Loss: 0.0095, Val Acc: 0.9592, Val F1: 0.9598, Val AUC: 0.9907\n",
            "Epoch 30/100 - Train Loss: 0.0046, Val Loss: 0.0102, Val Acc: 0.9610, Val F1: 0.9613, Val AUC: 0.9905\n",
            "Epoch 40/100 - Train Loss: 0.0046, Val Loss: 0.0114, Val Acc: 0.9622, Val F1: 0.9626, Val AUC: 0.9899\n",
            "Epoch 50/100 - Train Loss: 0.0030, Val Loss: 0.0128, Val Acc: 0.9610, Val F1: 0.9614, Val AUC: 0.9904\n",
            "Epoch 60/100 - Train Loss: 0.0019, Val Loss: 0.0141, Val Acc: 0.9610, Val F1: 0.9615, Val AUC: 0.9914\n",
            "Early stopping at epoch 67\n",
            "\n",
            "Fold 19 Results:\n",
            "Loss: 0.0147\n",
            "Accuracy: 96.28%\n",
            "Precision: 0.9529\n",
            "Recall: 0.9736\n",
            "F1 Score: 0.9631\n",
            "AUC: 0.9911\n",
            "\n",
            "============================================================\n",
            "Fold 20/30\n",
            "============================================================\n",
            "Epoch 10/100 - Train Loss: 0.0094, Val Loss: 0.0119, Val Acc: 0.9453, Val F1: 0.9469, Val AUC: 0.9858\n",
            "Epoch 20/100 - Train Loss: 0.0067, Val Loss: 0.0104, Val Acc: 0.9508, Val F1: 0.9515, Val AUC: 0.9879\n",
            "Epoch 30/100 - Train Loss: 0.0047, Val Loss: 0.0112, Val Acc: 0.9544, Val F1: 0.9550, Val AUC: 0.9893\n",
            "Epoch 40/100 - Train Loss: 0.0047, Val Loss: 0.0119, Val Acc: 0.9562, Val F1: 0.9571, Val AUC: 0.9876\n",
            "Epoch 50/100 - Train Loss: 0.0030, Val Loss: 0.0139, Val Acc: 0.9574, Val F1: 0.9579, Val AUC: 0.9886\n",
            "Epoch 60/100 - Train Loss: 0.0019, Val Loss: 0.0175, Val Acc: 0.9574, Val F1: 0.9579, Val AUC: 0.9886\n",
            "Epoch 70/100 - Train Loss: 0.0017, Val Loss: 0.0173, Val Acc: 0.9568, Val F1: 0.9572, Val AUC: 0.9891\n",
            "Early stopping at epoch 74\n",
            "\n",
            "Fold 20 Results:\n",
            "Loss: 0.0137\n",
            "Accuracy: 95.74%\n",
            "Precision: 0.9440\n",
            "Recall: 0.9724\n",
            "F1 Score: 0.9580\n",
            "AUC: 0.9891\n",
            "\n",
            "============================================================\n",
            "Fold 21/30\n",
            "============================================================\n",
            "Epoch 10/100 - Train Loss: 0.0095, Val Loss: 0.0112, Val Acc: 0.9483, Val F1: 0.9491, Val AUC: 0.9837\n",
            "Epoch 20/100 - Train Loss: 0.0065, Val Loss: 0.0155, Val Acc: 0.9544, Val F1: 0.9550, Val AUC: 0.9848\n",
            "Epoch 30/100 - Train Loss: 0.0046, Val Loss: 0.0146, Val Acc: 0.9586, Val F1: 0.9591, Val AUC: 0.9875\n",
            "Epoch 40/100 - Train Loss: 0.0047, Val Loss: 0.0161, Val Acc: 0.9538, Val F1: 0.9548, Val AUC: 0.9872\n",
            "Epoch 50/100 - Train Loss: 0.0031, Val Loss: 0.0209, Val Acc: 0.9616, Val F1: 0.9620, Val AUC: 0.9872\n",
            "Epoch 60/100 - Train Loss: 0.0020, Val Loss: 0.0293, Val Acc: 0.9622, Val F1: 0.9627, Val AUC: 0.9872\n",
            "Epoch 70/100 - Train Loss: 0.0016, Val Loss: 0.0304, Val Acc: 0.9616, Val F1: 0.9621, Val AUC: 0.9876\n",
            "Early stopping at epoch 74\n",
            "\n",
            "Fold 21 Results:\n",
            "Loss: 0.0156\n",
            "Accuracy: 95.62%\n",
            "Precision: 0.9470\n",
            "Recall: 0.9663\n",
            "F1 Score: 0.9566\n",
            "AUC: 0.9881\n",
            "\n",
            "============================================================\n",
            "Fold 22/30\n",
            "============================================================\n",
            "Epoch 10/100 - Train Loss: 0.0094, Val Loss: 0.0104, Val Acc: 0.9453, Val F1: 0.9469, Val AUC: 0.9841\n",
            "Epoch 20/100 - Train Loss: 0.0065, Val Loss: 0.0098, Val Acc: 0.9556, Val F1: 0.9567, Val AUC: 0.9884\n",
            "Epoch 30/100 - Train Loss: 0.0046, Val Loss: 0.0101, Val Acc: 0.9622, Val F1: 0.9628, Val AUC: 0.9898\n",
            "Epoch 40/100 - Train Loss: 0.0045, Val Loss: 0.0105, Val Acc: 0.9592, Val F1: 0.9598, Val AUC: 0.9891\n",
            "Early stopping at epoch 43\n",
            "\n",
            "Fold 22 Results:\n",
            "Loss: 0.0111\n",
            "Accuracy: 96.34%\n",
            "Precision: 0.9498\n",
            "Recall: 0.9784\n",
            "F1 Score: 0.9639\n",
            "AUC: 0.9893\n",
            "\n",
            "============================================================\n",
            "Fold 23/30\n",
            "============================================================\n",
            "Epoch 10/100 - Train Loss: 0.0095, Val Loss: 0.0124, Val Acc: 0.9508, Val F1: 0.9515, Val AUC: 0.9849\n",
            "Epoch 20/100 - Train Loss: 0.0063, Val Loss: 0.0118, Val Acc: 0.9592, Val F1: 0.9600, Val AUC: 0.9869\n",
            "Epoch 30/100 - Train Loss: 0.0046, Val Loss: 0.0126, Val Acc: 0.9628, Val F1: 0.9634, Val AUC: 0.9873\n",
            "Epoch 40/100 - Train Loss: 0.0045, Val Loss: 0.0135, Val Acc: 0.9592, Val F1: 0.9599, Val AUC: 0.9874\n",
            "Early stopping at epoch 44\n",
            "\n",
            "Fold 23 Results:\n",
            "Loss: 0.0147\n",
            "Accuracy: 95.98%\n",
            "Precision: 0.9463\n",
            "Recall: 0.9748\n",
            "F1 Score: 0.9603\n",
            "AUC: 0.9868\n",
            "\n",
            "============================================================\n",
            "Fold 24/30\n",
            "============================================================\n",
            "Epoch 10/100 - Train Loss: 0.0091, Val Loss: 0.0103, Val Acc: 0.9538, Val F1: 0.9542, Val AUC: 0.9892\n",
            "Epoch 20/100 - Train Loss: 0.0065, Val Loss: 0.0098, Val Acc: 0.9598, Val F1: 0.9602, Val AUC: 0.9900\n",
            "Epoch 30/100 - Train Loss: 0.0045, Val Loss: 0.0099, Val Acc: 0.9652, Val F1: 0.9654, Val AUC: 0.9905\n",
            "Epoch 40/100 - Train Loss: 0.0046, Val Loss: 0.0111, Val Acc: 0.9682, Val F1: 0.9685, Val AUC: 0.9886\n",
            "Epoch 50/100 - Train Loss: 0.0030, Val Loss: 0.0126, Val Acc: 0.9640, Val F1: 0.9644, Val AUC: 0.9899\n",
            "Epoch 60/100 - Train Loss: 0.0019, Val Loss: 0.0141, Val Acc: 0.9700, Val F1: 0.9702, Val AUC: 0.9901\n",
            "Epoch 70/100 - Train Loss: 0.0016, Val Loss: 0.0161, Val Acc: 0.9712, Val F1: 0.9715, Val AUC: 0.9899\n",
            "Early stopping at epoch 76\n",
            "\n",
            "Fold 24 Results:\n",
            "Loss: 0.0130\n",
            "Accuracy: 97.00%\n",
            "Precision: 0.9568\n",
            "Recall: 0.9844\n",
            "F1 Score: 0.9704\n",
            "AUC: 0.9895\n",
            "\n",
            "============================================================\n",
            "Fold 25/30\n",
            "============================================================\n",
            "Epoch 10/100 - Train Loss: 0.0093, Val Loss: 0.0079, Val Acc: 0.9556, Val F1: 0.9563, Val AUC: 0.9898\n",
            "Epoch 20/100 - Train Loss: 0.0064, Val Loss: 0.0081, Val Acc: 0.9604, Val F1: 0.9610, Val AUC: 0.9915\n",
            "Epoch 30/100 - Train Loss: 0.0046, Val Loss: 0.0078, Val Acc: 0.9646, Val F1: 0.9648, Val AUC: 0.9931\n",
            "Epoch 40/100 - Train Loss: 0.0046, Val Loss: 0.0091, Val Acc: 0.9622, Val F1: 0.9620, Val AUC: 0.9921\n",
            "Epoch 50/100 - Train Loss: 0.0031, Val Loss: 0.0090, Val Acc: 0.9664, Val F1: 0.9666, Val AUC: 0.9937\n",
            "Epoch 60/100 - Train Loss: 0.0020, Val Loss: 0.0103, Val Acc: 0.9658, Val F1: 0.9656, Val AUC: 0.9937\n",
            "Epoch 70/100 - Train Loss: 0.0016, Val Loss: 0.0115, Val Acc: 0.9658, Val F1: 0.9657, Val AUC: 0.9935\n",
            "Early stopping at epoch 71\n",
            "\n",
            "Fold 25 Results:\n",
            "Loss: 0.0116\n",
            "Accuracy: 96.34%\n",
            "Precision: 0.9661\n",
            "Recall: 0.9603\n",
            "F1 Score: 0.9632\n",
            "AUC: 0.9922\n",
            "\n",
            "============================================================\n",
            "Fold 26/30\n",
            "============================================================\n",
            "Epoch 10/100 - Train Loss: 0.0093, Val Loss: 0.0085, Val Acc: 0.9514, Val F1: 0.9521, Val AUC: 0.9891\n",
            "Epoch 20/100 - Train Loss: 0.0065, Val Loss: 0.0080, Val Acc: 0.9556, Val F1: 0.9564, Val AUC: 0.9908\n",
            "Epoch 30/100 - Train Loss: 0.0046, Val Loss: 0.0096, Val Acc: 0.9622, Val F1: 0.9627, Val AUC: 0.9914\n",
            "Epoch 40/100 - Train Loss: 0.0046, Val Loss: 0.0094, Val Acc: 0.9616, Val F1: 0.9618, Val AUC: 0.9906\n",
            "Epoch 50/100 - Train Loss: 0.0030, Val Loss: 0.0129, Val Acc: 0.9634, Val F1: 0.9637, Val AUC: 0.9909\n",
            "Epoch 60/100 - Train Loss: 0.0020, Val Loss: 0.0145, Val Acc: 0.9622, Val F1: 0.9627, Val AUC: 0.9919\n",
            "Epoch 70/100 - Train Loss: 0.0016, Val Loss: 0.0151, Val Acc: 0.9634, Val F1: 0.9637, Val AUC: 0.9924\n",
            "Early stopping at epoch 74\n",
            "\n",
            "Fold 26 Results:\n",
            "Loss: 0.0118\n",
            "Accuracy: 96.22%\n",
            "Precision: 0.9497\n",
            "Recall: 0.9760\n",
            "F1 Score: 0.9627\n",
            "AUC: 0.9917\n",
            "\n",
            "============================================================\n",
            "Fold 27/30\n",
            "============================================================\n",
            "Epoch 10/100 - Train Loss: 0.0096, Val Loss: 0.0100, Val Acc: 0.9477, Val F1: 0.9489, Val AUC: 0.9857\n",
            "Epoch 20/100 - Train Loss: 0.0066, Val Loss: 0.0093, Val Acc: 0.9580, Val F1: 0.9589, Val AUC: 0.9891\n",
            "Epoch 30/100 - Train Loss: 0.0048, Val Loss: 0.0102, Val Acc: 0.9628, Val F1: 0.9633, Val AUC: 0.9898\n",
            "Epoch 40/100 - Train Loss: 0.0048, Val Loss: 0.0107, Val Acc: 0.9622, Val F1: 0.9627, Val AUC: 0.9891\n",
            "Early stopping at epoch 41\n",
            "\n",
            "Fold 27 Results:\n",
            "Loss: 0.0113\n",
            "Accuracy: 96.10%\n",
            "Precision: 0.9454\n",
            "Recall: 0.9784\n",
            "F1 Score: 0.9616\n",
            "AUC: 0.9894\n",
            "\n",
            "============================================================\n",
            "Fold 28/30\n",
            "============================================================\n",
            "Epoch 10/100 - Train Loss: 0.0094, Val Loss: 0.0082, Val Acc: 0.9532, Val F1: 0.9546, Val AUC: 0.9895\n",
            "Epoch 20/100 - Train Loss: 0.0063, Val Loss: 0.0088, Val Acc: 0.9556, Val F1: 0.9567, Val AUC: 0.9904\n",
            "Epoch 30/100 - Train Loss: 0.0044, Val Loss: 0.0106, Val Acc: 0.9592, Val F1: 0.9600, Val AUC: 0.9900\n",
            "Epoch 40/100 - Train Loss: 0.0044, Val Loss: 0.0106, Val Acc: 0.9592, Val F1: 0.9600, Val AUC: 0.9909\n",
            "Early stopping at epoch 48\n",
            "\n",
            "Fold 28 Results:\n",
            "Loss: 0.0136\n",
            "Accuracy: 95.56%\n",
            "Precision: 0.9417\n",
            "Recall: 0.9712\n",
            "F1 Score: 0.9562\n",
            "AUC: 0.9886\n",
            "\n",
            "============================================================\n",
            "Fold 29/30\n",
            "============================================================\n",
            "Epoch 10/100 - Train Loss: 0.0097, Val Loss: 0.0088, Val Acc: 0.9477, Val F1: 0.9484, Val AUC: 0.9875\n",
            "Epoch 20/100 - Train Loss: 0.0066, Val Loss: 0.0081, Val Acc: 0.9610, Val F1: 0.9611, Val AUC: 0.9910\n",
            "Epoch 30/100 - Train Loss: 0.0048, Val Loss: 0.0093, Val Acc: 0.9646, Val F1: 0.9649, Val AUC: 0.9911\n",
            "Epoch 40/100 - Train Loss: 0.0049, Val Loss: 0.0084, Val Acc: 0.9640, Val F1: 0.9642, Val AUC: 0.9924\n",
            "Epoch 50/100 - Train Loss: 0.0032, Val Loss: 0.0101, Val Acc: 0.9712, Val F1: 0.9715, Val AUC: 0.9930\n",
            "Epoch 60/100 - Train Loss: 0.0021, Val Loss: 0.0125, Val Acc: 0.9718, Val F1: 0.9721, Val AUC: 0.9928\n",
            "Epoch 70/100 - Train Loss: 0.0017, Val Loss: 0.0121, Val Acc: 0.9754, Val F1: 0.9756, Val AUC: 0.9932\n",
            "Early stopping at epoch 76\n",
            "\n",
            "Fold 29 Results:\n",
            "Loss: 0.0112\n",
            "Accuracy: 96.94%\n",
            "Precision: 0.9643\n",
            "Recall: 0.9748\n",
            "F1 Score: 0.9695\n",
            "AUC: 0.9922\n",
            "\n",
            "============================================================\n",
            "Fold 30/30\n",
            "============================================================\n",
            "Epoch 10/100 - Train Loss: 0.0095, Val Loss: 0.0094, Val Acc: 0.9526, Val F1: 0.9536, Val AUC: 0.9876\n",
            "Epoch 20/100 - Train Loss: 0.0067, Val Loss: 0.0093, Val Acc: 0.9574, Val F1: 0.9582, Val AUC: 0.9898\n",
            "Epoch 30/100 - Train Loss: 0.0047, Val Loss: 0.0098, Val Acc: 0.9622, Val F1: 0.9628, Val AUC: 0.9912\n",
            "Epoch 40/100 - Train Loss: 0.0047, Val Loss: 0.0098, Val Acc: 0.9610, Val F1: 0.9615, Val AUC: 0.9905\n",
            "Early stopping at epoch 41\n",
            "\n",
            "Fold 30 Results:\n",
            "Loss: 0.0096\n",
            "Accuracy: 96.28%\n",
            "Precision: 0.9477\n",
            "Recall: 0.9796\n",
            "F1 Score: 0.9634\n",
            "AUC: 0.9915\n",
            "\n",
            "======================================================================\n",
            "IMPROVED MODEL RESULTS\n",
            "======================================================================\n",
            " Fold     Loss  Accuracy (%)  Precision   Recall  F1 Score      AUC\n",
            "    1 0.015670     95.138055   0.934180 0.971188  0.952325 0.988106\n",
            "    2 0.017516     95.498199   0.947991 0.962785  0.955331 0.990484\n",
            "    3 0.019618     96.158463   0.952886 0.971188  0.961950 0.986104\n",
            "    4 0.014471     95.618247   0.940835 0.973589  0.956932 0.987459\n",
            "    5 0.009550     96.758703   0.966427 0.968750  0.967587 0.990939\n",
            "    6 0.015187     95.798319   0.945093 0.972356  0.958531 0.989026\n",
            "    7 0.013152     96.038415   0.945349 0.977163  0.960993 0.990866\n",
            "    8 0.018940     94.717887   0.920814 0.978365  0.948718 0.984230\n",
            "    9 0.008745     95.918367   0.943155 0.977163  0.959858 0.991235\n",
            "   10 0.016815     95.618247   0.936709 0.978365  0.957084 0.989540\n",
            "   11 0.026633     96.218487   0.952886 0.972356  0.962522 0.990444\n",
            "   12 0.008157     96.638655   0.965228 0.967548  0.966387 0.992492\n",
            "   13 0.008516     96.278511   0.957245 0.968750  0.962963 0.991954\n",
            "   14 0.011569     95.678271   0.941860 0.973558  0.957447 0.990996\n",
            "   15 0.009576     96.276276   0.952941 0.973558  0.963139 0.989610\n",
            "   16 0.031303     95.555556   0.942757 0.969952  0.956161 0.989979\n",
            "   17 0.016024     95.675676   0.947059 0.967548  0.957194 0.987865\n",
            "   18 0.012605     95.795796   0.947183 0.969952  0.958432 0.988810\n",
            "   19 0.014676     96.276276   0.952941 0.973558  0.963139 0.991054\n",
            "   20 0.013666     95.735736   0.943991 0.972356  0.957963 0.989083\n",
            "   21 0.015626     95.615616   0.946996 0.966346  0.956573 0.988057\n",
            "   22 0.011114     96.336336   0.949825 0.978365  0.963884 0.989301\n",
            "   23 0.014729     95.975976   0.946324 0.974760  0.960332 0.986831\n",
            "   24 0.013006     96.996997   0.956776 0.984375  0.970379 0.989455\n",
            "   25 0.011569     96.336336   0.966143 0.960337  0.963231 0.992223\n",
            "   26 0.011830     96.216216   0.949708 0.975962  0.962656 0.991739\n",
            "   27 0.011313     96.096096   0.945412 0.978365  0.961607 0.989354\n",
            "   28 0.013631     95.555556   0.941725 0.971154  0.956213 0.988598\n",
            "   29 0.011212     96.936937   0.964328 0.974760  0.969516 0.992220\n",
            "   30 0.009606     96.276276   0.947674 0.979567  0.963357 0.991540\n",
            "\n",
            "======================================================================\n",
            "AVERAGE METRICS\n",
            "======================================================================\n",
            "Loss: 0.0142\n",
            "Accuracy: 95.99%\n",
            "Precision: 0.9484\n",
            "Recall: 0.9728\n",
            "F1 Score: 0.9604\n",
            "AUC: 0.9897\n",
            "\n",
            "======================================================================\n",
            "COMPARISON WITH ORIGINAL\n",
            "======================================================================\n",
            "Original Paper: 90.14% accuracy\n",
            "First Implementation: 85.04% accuracy\n",
            "Improved Model: 95.99% accuracy\n",
            "Improvement: +10.95%\n",
            "\n",
            "======================================================================\n",
            "BEST FOLD: Fold 24\n",
            "======================================================================\n",
            "Accuracy: 97.00%\n",
            "Precision: 0.9568\n",
            "Recall: 0.9844\n",
            "F1 Score: 0.9704\n",
            "AUC: 0.9895\n",
            "\n",
            "✓ Best model saved as 'best_model_improved.pt'\n",
            "\n",
            "✓ Training data saved as 'training_data.pkl'\n",
            "  You can regenerate/edit visualizations without retraining by loading this file\n",
            "\n",
            "======================================================================\n",
            "GENERATING VISUALIZATIONS\n",
            "======================================================================\n",
            "✓ Saved: plots/accuracy_curve.png\n",
            "✓ Saved: plots/loss_curves.png\n",
            "✓ Saved: plots/f1_score_curve.png\n",
            "✓ Saved: plots/confusion_matrix_normalized.png\n",
            "✓ Saved: plots/confusion_matrix_counts.png\n",
            "✓ Saved: plots/all_folds_comparison.png\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3731764925.py:160: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n",
            "  bp = plt.boxplot(metrics_data, labels=['Accuracy', 'Precision', 'Recall', 'F1 Score', 'AUC'],\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Saved: plots/metrics_distribution.png\n",
            "\n",
            "✓ All visualizations saved in 'plots/' directory\n",
            "  Files created:\n",
            "    - accuracy_curve.png\n",
            "    - loss_curves.png\n",
            "    - f1_score_curve.png\n",
            "    - confusion_matrix_normalized.png\n",
            "    - confusion_matrix_counts.png\n",
            "    - all_folds_comparison.png\n",
            "    - metrics_distribution.png\n",
            "\n",
            "======================================================================\n",
            "PIPELINE COMPLETE!\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "def save_training_data(fold_results: List[Dict], best_fold_idx: int, filename: str = 'training_data.pkl'):\n",
        "    \"\"\"Save training data for later visualization\"\"\"\n",
        "    import pickle\n",
        "\n",
        "    training_data = {\n",
        "        'fold_results': fold_results,\n",
        "        'best_fold_idx': best_fold_idx,\n",
        "        'timestamp': pd.Timestamp.now().isoformat(),\n",
        "        'num_folds': len(fold_results),\n",
        "        'summary_metrics': {\n",
        "            'mean_accuracy': np.mean([r['accuracy'] for r in fold_results]),\n",
        "            'mean_precision': np.mean([r['precision'] for r in fold_results]),\n",
        "            'mean_recall': np.mean([r['recall'] for r in fold_results]),\n",
        "            'mean_f1': np.mean([r['f1_score'] for r in fold_results]),\n",
        "            'mean_auc': np.mean([r['auc'] for r in fold_results]),\n",
        "            'std_accuracy': np.std([r['accuracy'] for r in fold_results]),\n",
        "        }\n",
        "    }\n",
        "\n",
        "    with open(filename, 'wb') as f:\n",
        "        pickle.dump(training_data, f)\n",
        "\n",
        "    print(f\"\\n✓ Training data saved as '{filename}'\")\n",
        "    print(f\"  You can regenerate/edit visualizations without retraining by loading this file\")\n",
        "    return training_data\n",
        "\n",
        "\n",
        "def plot_improved_results(fold_results: List[Dict], best_fold_idx: int):\n",
        "    \"\"\"Plot training curves and confusion matrix as separate PNG files\"\"\"\n",
        "    import os\n",
        "    best_fold = fold_results[best_fold_idx]\n",
        "    epochs = range(1, len(best_fold['val_accuracies']) + 1)\n",
        "\n",
        "    # Create output directory\n",
        "    os.makedirs('plots', exist_ok=True)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"GENERATING VISUALIZATIONS\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # 1. Accuracy curve\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(epochs, [acc * 100 for acc in best_fold['val_accuracies']],\n",
        "             'b-', linewidth=2.5, label='Validation Accuracy', marker='o', markersize=4)\n",
        "    plt.xlabel('Epoch', fontsize=14)\n",
        "    plt.ylabel('Accuracy (%)', fontsize=14)\n",
        "    plt.title(f'Validation Accuracy - Fold {best_fold[\"fold\"]} (Best: {best_fold[\"accuracy\"]:.2f}%)',\n",
        "              fontsize=16, fontweight='bold')\n",
        "    plt.legend(fontsize=12)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('plots/accuracy_curve.png', dpi=300, bbox_inches='tight')\n",
        "    print(\"✓ Saved: plots/accuracy_curve.png\")\n",
        "    plt.close()\n",
        "\n",
        "    # 2. Loss curves\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(epochs, best_fold['train_losses'], 'r-', linewidth=2.5,\n",
        "             label='Training Loss', marker='s', markersize=4)\n",
        "    plt.plot(epochs, best_fold['val_losses'], 'b-', linewidth=2.5,\n",
        "             label='Validation Loss', marker='o', markersize=4)\n",
        "    plt.xlabel('Epoch', fontsize=14)\n",
        "    plt.ylabel('Loss', fontsize=14)\n",
        "    plt.title(f'Training Curves - Fold {best_fold[\"fold\"]}',\n",
        "              fontsize=16, fontweight='bold')\n",
        "    plt.legend(fontsize=12)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('plots/loss_curves.png', dpi=300, bbox_inches='tight')\n",
        "    print(\"✓ Saved: plots/loss_curves.png\")\n",
        "    plt.close()\n",
        "\n",
        "    # 3. F1 Score curve\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(epochs, best_fold['val_f1_scores'], 'g-', linewidth=2.5,\n",
        "             label='F1 Score', marker='^', markersize=4)\n",
        "    plt.xlabel('Epoch', fontsize=14)\n",
        "    plt.ylabel('F1 Score', fontsize=14)\n",
        "    plt.title(f'F1 Score - Fold {best_fold[\"fold\"]} (Best: {best_fold[\"f1_score\"]:.4f})',\n",
        "              fontsize=16, fontweight='bold')\n",
        "    plt.legend(fontsize=12)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.ylim(0, 1.05)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('plots/f1_score_curve.png', dpi=300, bbox_inches='tight')\n",
        "    print(\"✓ Saved: plots/f1_score_curve.png\")\n",
        "    plt.close()\n",
        "\n",
        "    # 4. Confusion Matrix (normalized)\n",
        "    cm = confusion_matrix(best_fold['true_labels'], best_fold['predictions'])\n",
        "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    plt.figure(figsize=(8, 7))\n",
        "    sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Blues',\n",
        "                xticklabels=['Non-BacLAB', 'BacLAB'],\n",
        "                yticklabels=['Non-BacLAB', 'BacLAB'],\n",
        "                cbar_kws={'label': 'Percentage'})\n",
        "    plt.xlabel('Predicted', fontsize=14)\n",
        "    plt.ylabel('True', fontsize=14)\n",
        "    plt.title(f'Confusion Matrix - Fold {best_fold[\"fold\"]} (Normalized)',\n",
        "              fontsize=16, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('plots/confusion_matrix_normalized.png', dpi=300, bbox_inches='tight')\n",
        "    print(\"✓ Saved: plots/confusion_matrix_normalized.png\")\n",
        "    plt.close()\n",
        "\n",
        "    # 5. Confusion Matrix (raw counts)\n",
        "    plt.figure(figsize=(8, 7))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Oranges',\n",
        "                xticklabels=['Non-BacLAB', 'BacLAB'],\n",
        "                yticklabels=['Non-BacLAB', 'BacLAB'],\n",
        "                cbar_kws={'label': 'Count'})\n",
        "    plt.xlabel('Predicted', fontsize=14)\n",
        "    plt.ylabel('True', fontsize=14)\n",
        "    plt.title(f'Confusion Matrix - Fold {best_fold[\"fold\"]} (Counts)',\n",
        "              fontsize=16, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('plots/confusion_matrix_counts.png', dpi=300, bbox_inches='tight')\n",
        "    print(\"✓ Saved: plots/confusion_matrix_counts.png\")\n",
        "    plt.close()\n",
        "\n",
        "    # 6. All metrics comparison across folds\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    fold_nums = [r['fold'] for r in fold_results]\n",
        "    accuracies = [r['accuracy'] for r in fold_results]\n",
        "    precisions = [r['precision'] * 100 for r in fold_results]\n",
        "    recalls = [r['recall'] * 100 for r in fold_results]\n",
        "    f1_scores = [r['f1_score'] * 100 for r in fold_results]\n",
        "\n",
        "    x = np.arange(len(fold_nums))\n",
        "    width = 0.2\n",
        "\n",
        "    plt.bar(x - 1.5*width, accuracies, width, label='Accuracy', alpha=0.8)\n",
        "    plt.bar(x - 0.5*width, precisions, width, label='Precision', alpha=0.8)\n",
        "    plt.bar(x + 0.5*width, recalls, width, label='Recall', alpha=0.8)\n",
        "    plt.bar(x + 1.5*width, f1_scores, width, label='F1 Score', alpha=0.8)\n",
        "\n",
        "    plt.xlabel('Fold', fontsize=14)\n",
        "    plt.ylabel('Score (%)', fontsize=14)\n",
        "    plt.title('Performance Metrics Across All Folds', fontsize=16, fontweight='bold')\n",
        "    plt.xticks(x, fold_nums)\n",
        "    plt.legend(fontsize=11)\n",
        "    plt.grid(True, alpha=0.3, axis='y')\n",
        "    plt.ylim(85, 100)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('plots/all_folds_comparison.png', dpi=300, bbox_inches='tight')\n",
        "    print(\"✓ Saved: plots/all_folds_comparison.png\")\n",
        "    plt.close()\n",
        "\n",
        "    # 7. Box plot of metrics across folds\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    metrics_data = [\n",
        "        [r['accuracy'] for r in fold_results],\n",
        "        [r['precision'] * 100 for r in fold_results],\n",
        "        [r['recall'] * 100 for r in fold_results],\n",
        "        [r['f1_score'] * 100 for r in fold_results],\n",
        "        [r['auc'] * 100 for r in fold_results]\n",
        "    ]\n",
        "\n",
        "    bp = plt.boxplot(metrics_data, labels=['Accuracy', 'Precision', 'Recall', 'F1 Score', 'AUC'],\n",
        "                     patch_artist=True, showmeans=True)\n",
        "\n",
        "    colors = ['lightblue', 'lightgreen', 'lightyellow', 'lightcoral', 'plum']\n",
        "    for patch, color in zip(bp['boxes'], colors):\n",
        "        patch.set_facecolor(color)\n",
        "\n",
        "    plt.ylabel('Score (%)', fontsize=14)\n",
        "    plt.title('Distribution of Metrics Across All Folds', fontsize=16, fontweight='bold')\n",
        "    plt.grid(True, alpha=0.3, axis='y')\n",
        "    plt.ylim(85, 100)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('plots/metrics_distribution.png', dpi=300, bbox_inches='tight')\n",
        "    print(\"✓ Saved: plots/metrics_distribution.png\")\n",
        "    plt.close()\n",
        "\n",
        "    print(\"\\n✓ All visualizations saved in 'plots/' directory\")\n",
        "    print(\"  Files created:\")\n",
        "    print(\"    - accuracy_curve.png\")\n",
        "    print(\"    - loss_curves.png\")\n",
        "    print(\"    - f1_score_curve.png\")\n",
        "    print(\"    - confusion_matrix_normalized.png\")\n",
        "    print(\"    - confusion_matrix_counts.png\")\n",
        "    print(\"    - all_folds_comparison.png\")\n",
        "    print(\"    - metrics_distribution.png\")\n",
        "\n",
        "\n",
        "def main_improved():\n",
        "    \"\"\"\n",
        "    Improved main pipeline with state-of-the-art techniques\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"=\"*70)\n",
        "    print(\"BacLABNet v2.0: Improved Bacteriocin Classification\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Set random seeds for reproducibility\n",
        "    torch.manual_seed(42)\n",
        "    np.random.seed(42)\n",
        "    random.seed(42)\n",
        "\n",
        "    # 1. Load data\n",
        "    print(\"\\n[1/6] Loading data...\")\n",
        "    df = pd.read_csv('data_BacLAB_and_nonBacLAB.csv',\n",
        "                     header=None,\n",
        "                     names=['ID', 'Species', 'Sequence', 'Label', 'Empty'])\n",
        "\n",
        "    sequences = df['Sequence'].tolist()\n",
        "    labels = df['Label'].values\n",
        "\n",
        "    print(f\"Total sequences: {len(sequences)}\")\n",
        "    print(f\"BacLAB: {sum(labels)}, Non-BacLAB: {len(labels) - sum(labels)}\")\n",
        "\n",
        "    # 2. Extract improved k-mer features (with TF-IDF)\n",
        "    print(\"\\n[2/6] Extracting improved k-mer features (TF-IDF)...\")\n",
        "\n",
        "    try:\n",
        "        kmers_df = pd.read_csv('List_kmers.csv')\n",
        "        kmers_5 = kmers_df['5-mers'].dropna().tolist()[:150]  # Top 150 instead of 100\n",
        "        kmers_7 = kmers_df['7-mers'].dropna().tolist()[:150]\n",
        "        print(\"Loaded pre-computed k-mers\")\n",
        "    except:\n",
        "        print(\"Computing k-mers...\")\n",
        "        baclab_sequences = [seq for seq, lbl in zip(sequences, labels) if lbl == 1]\n",
        "        kmers_5 = get_top_kmers(baclab_sequences, k=5, top_n=150)\n",
        "        kmers_7 = get_top_kmers(baclab_sequences, k=7, top_n=150)\n",
        "\n",
        "    # Calculate IDF weights\n",
        "    print(\"Calculating TF-IDF weights...\")\n",
        "    idf_5 = calculate_idf_weights(sequences, kmers_5, 5)\n",
        "    idf_7 = calculate_idf_weights(sequences, kmers_7, 7)\n",
        "\n",
        "    features_5 = np.array([extract_kmer_features_tfidf(seq, kmers_5, 5, idf_5) for seq in sequences])\n",
        "    features_7 = np.array([extract_kmer_features_tfidf(seq, kmers_7, 7, idf_7) for seq in sequences])\n",
        "\n",
        "    print(f\"5-mer TF-IDF features: {features_5.shape}\")\n",
        "    print(f\"7-mer TF-IDF features: {features_7.shape}\")\n",
        "\n",
        "    # 3. Extract physicochemical features\n",
        "    print(\"\\n[3/6] Extracting physicochemical features...\")\n",
        "    start_time = time.time()\n",
        "    physchem_features = np.array([extract_physicochemical_features(seq) for seq in sequences])\n",
        "    print(f\"Physicochemical features: {physchem_features.shape}\")\n",
        "    print(f\"Time: {time.time() - start_time:.2f} seconds\")\n",
        "\n",
        "    # 4. Load pre-computed embeddings (or use simpler features if not available)\n",
        "    print(\"\\n[4/6] Loading embedding features...\")\n",
        "    embedding_features = None\n",
        "\n",
        "    # Try ESM-2 embeddings first (best quality), then fall back to GRU embeddings\n",
        "    try:\n",
        "        embedding_features = np.load('esm2_embeddings.npy')\n",
        "        print(f\"✓ Loaded ESM-2 embeddings: {embedding_features.shape}\")\n",
        "        print(\"  (Using state-of-the-art ESM-2 protein language model embeddings)\")\n",
        "    except:\n",
        "        try:\n",
        "            embedding_features = np.load('embeddings.npy')\n",
        "            print(f\"✓ Loaded embeddings: {embedding_features.shape}\")\n",
        "            print(\"  (Using GRU embeddings - for best results, run esm2_embeddings.py)\")\n",
        "        except:\n",
        "            print(\"⚠ No pre-computed embeddings found. Using only k-mers and physicochemical features.\")\n",
        "            print(\"  → For best results (+5-8% accuracy), run: python esm2_embeddings.py\")\n",
        "            embedding_features = np.zeros((len(sequences), 0))\n",
        "\n",
        "    # 5. Concatenate all features\n",
        "    print(\"\\n[5/6] Concatenating features...\")\n",
        "    if embedding_features.shape[1] > 0:\n",
        "        features = np.concatenate([features_5, features_7, physchem_features, embedding_features], axis=1)\n",
        "    else:\n",
        "        features = np.concatenate([features_5, features_7, physchem_features], axis=1)\n",
        "\n",
        "    print(f\"Final features shape: {features.shape}\")\n",
        "    print(f\"Feature breakdown:\")\n",
        "    print(f\"  - 5-mer TF-IDF: {features_5.shape[1]}\")\n",
        "    print(f\"  - 7-mer TF-IDF: {features_7.shape[1]}\")\n",
        "    print(f\"  - Physicochemical: {physchem_features.shape[1]}\")\n",
        "    if embedding_features.shape[1] > 0:\n",
        "        print(f\"  - Embeddings: {embedding_features.shape[1]}\")\n",
        "    else:\n",
        "        print(f\"  - Embeddings: 0 (not used)\")\n",
        "    print(f\"  - Total: {features.shape[1]}\")\n",
        "\n",
        "    # 6. Train with improved k-fold cross-validation\n",
        "    print(\"\\n[6/6] Starting improved k-fold cross-validation...\")\n",
        "    print(\"Improvements:\")\n",
        "    print(\"  ✓ Transformer architecture with multi-head attention\")\n",
        "    print(\"  ✓ Focal loss for better hard example learning\")\n",
        "    print(\"  ✓ Cosine annealing learning rate schedule\")\n",
        "    print(\"  ✓ Early stopping with patience=15\")\n",
        "    print(\"  ✓ Gradient clipping\")\n",
        "    print(\"  ✓ Deeper network (512→256→128→64→32→2)\")\n",
        "\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    print(f\"  Device: {device.upper()}\")\n",
        "\n",
        "    fold_results = kfold_cross_validation_improved(\n",
        "        features=features,\n",
        "        labels=labels,\n",
        "        sequences=sequences,\n",
        "        k=30,  # Using 30 folds to match original paper\n",
        "        epochs=100,\n",
        "        batch_size=64,\n",
        "        learning_rate=1e-4,\n",
        "        device=device,\n",
        "        patience=15\n",
        "    )\n",
        "\n",
        "    # 7. Analyze results\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"IMPROVED MODEL RESULTS\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    results_df = pd.DataFrame([{\n",
        "        'Fold': r['fold'],\n",
        "        'Loss': r['loss'],\n",
        "        'Accuracy (%)': r['accuracy'],\n",
        "        'Precision': r['precision'],\n",
        "        'Recall': r['recall'],\n",
        "        'F1 Score': r['f1_score'],\n",
        "        'AUC': r['auc']\n",
        "    } for r in fold_results])\n",
        "\n",
        "    print(results_df.to_string(index=False))\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"AVERAGE METRICS\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Loss: {results_df['Loss'].mean():.4f}\")\n",
        "    print(f\"Accuracy: {results_df['Accuracy (%)'].mean():.2f}%\")\n",
        "    print(f\"Precision: {results_df['Precision'].mean():.4f}\")\n",
        "    print(f\"Recall: {results_df['Recall'].mean():.4f}\")\n",
        "    print(f\"F1 Score: {results_df['F1 Score'].mean():.4f}\")\n",
        "    print(f\"AUC: {results_df['AUC'].mean():.4f}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"COMPARISON WITH ORIGINAL\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Original Paper: 90.14% accuracy\")\n",
        "    print(f\"First Implementation: 85.04% accuracy\")\n",
        "    print(f\"Improved Model: {results_df['Accuracy (%)'].mean():.2f}% accuracy\")\n",
        "    print(f\"Improvement: {results_df['Accuracy (%)'].mean() - 90.14:+.2f}%\")\n",
        "\n",
        "    # Find best fold\n",
        "    best_fold_idx = results_df['F1 Score'].idxmax()\n",
        "    best_fold = fold_results[best_fold_idx]\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"BEST FOLD: Fold {best_fold['fold']}\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Accuracy: {best_fold['accuracy']:.2f}%\")\n",
        "    print(f\"Precision: {best_fold['precision']:.4f}\")\n",
        "    print(f\"Recall: {best_fold['recall']:.4f}\")\n",
        "    print(f\"F1 Score: {best_fold['f1_score']:.4f}\")\n",
        "    print(f\"AUC: {best_fold['auc']:.4f}\")\n",
        "\n",
        "    # Save best model\n",
        "    torch.save(best_fold['model_state'], 'best_model_improved.pt')\n",
        "    print(f\"\\n✓ Best model saved as 'best_model_improved.pt'\")\n",
        "\n",
        "    # Save training data for later visualization\n",
        "    save_training_data(fold_results, best_fold_idx, 'training_data.pkl')\n",
        "\n",
        "    # Plot comparison\n",
        "    plot_improved_results(fold_results, best_fold_idx)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"PIPELINE COMPLETE!\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    return fold_results, results_df\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main_improved()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mo1IxNA4_PDD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}